# 线性回归

## 代价函数 Cost Fuction

$$
J_{(\theta_{0}, \theta_{1})} = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}x^{(i)} - y^{(i)})^{2} 
$$

它的目标是: **选择出可以使得建模误差的平方和能够最小的模型参数**


## 批量梯度下降 Batch Gradient Descent

$$
repeat \quad \{ \\
\quad \quad  \quad \quad  \quad \quad  \quad \quad \quad \quad  \quad \quad  \theta_{j} :=   \theta_{j}  - \alpha\frac{\partial}{\partial\theta_{j}}J_{(\theta_{0}, \theta_{1})}
\\
\quad \quad  \quad \quad  \quad \quad  \quad \quad \quad  \quad \quad  \quad  \quad for \quad j = 1 \quad and \quad j = 0
\\
\}
$$

>**$\alpha$是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。**

>对于$\alpha$的取值可以尝试:  $\alpha =  0.01, 0.01, 0.1, 0.3, 1, 3, 10$

运用梯度下降法，关键在于求出代价函数的导数,即:
$$
\frac{\partial}{\partial\theta_{j}}J_{(\theta_{0}, \theta_{1})} = \frac{\partial}{\partial\theta_{j}}\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}
$$

>$j = 0$时:

$$
\frac{\partial}{\partial\theta_{j}}J_{(\theta_{0}, \theta_{1})}  = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})
$$

>$j = 1$时:

$$
\frac{\partial}{\partial\theta_{j}}J_{(\theta_{0}, \theta_{1})}  = \frac{1}{m}\sum_{i=1}^{m}((h_{\theta}(x^{(i)})-y^{(i)})x^{(i)})
$$


### 特征缩放 Feature Scaling （标准方程法）

>保证特征都具有相近的尺度，可以帮助梯度下降算法更快地收敛

$$
x_{n} = \frac{x_{n} - \mu_{n}}{s_{n}}
$$

**其中$\mu_{n}$是平均值，$s_{n}$是标准差**

## 正规方程 Normal Equation

$$
\theta = (X^{T}X)^{-1}X^{T}y
$$

**对于线性模型，在特征变量的数目并不大的情况下，标准方程是一个很好的计算参数的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。**

# 逻辑回归

逻辑回归算法是分类算法，我们将它作为分类算法使用。它适用于标签取值离散的情况，如：1 0 0 1。

## 假设函数 Hypothesis Function

$$
h_{\theta}(x) = g(\theta^{T}x)
$$

$$
z = \theta^{T}x
$$

$$
g(z) =  \frac{1}{1 + e^{-z}}
$$


## 代价函数 Cost Fuction

$$
J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}[ y^{(i)}\log(h_{\theta}(x^{(i)})) +(1-y^{(i)})\log(1 - h_{\theta}(x^{(i)}))]
$$


## 梯度下降

与线性回归一样，同样可以使用**批量梯度下降算法**来求得是代价函数最小的$\theta$:

Repeat {

$\theta_{j} :=   \theta_{j}  - \alpha\frac{1}{m}\sum_{i=1}^{m}((h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)})$

}

相比于线性回归，逻辑回归的预测函数(hypothesis function)$h_{\theta}(x) = g(\theta^{T}x)$是完全不同的。

## 多类别分类: 一对多

可以利用逻辑回归来实现一对多分类，具体思想是:

我们将多个类中的一个类标记为正向类$(y=1)$，然后将其他所有类都标记为负向类，这个模型记作$h_{\theta}^{(1)}(x)$。接着，类似地第我们选择另一个类标记为正向类$(y=2)$，再将其它类都标记为负向类，将这个模型记作 $h_{\theta}^{(2)}(x)$,依此类推。 最后我们得到一系列的模型简记为:$h_{\theta}^{(i)}(x) = p(y=i|x;\theta)$ 其中：$i=(1,2,3...k)$

最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。

# 正则化 Regularization

正则化是用来解决模型过拟合(OverFiting)问题的一种方法。它可以保留所有的特征，但是减少参数的大小。

>另一种方法是:丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如PCA）


## 正则化线性回归

我们需要在梯度下降是引入正则化项 $\lambda$

>$j=1,2...n时$

$$
   {\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}+\frac{\lambda }{m}{\theta_j}]
$$

简化这个公式:

$$
   {\theta_j}:={\theta_j}(1-a\frac{\lambda }{m})-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left(i\right)}}
$$

**可以看出正则化线性回归的梯度下降算法的变化在于:每次都在原有算法更新规则的基础上令$\theta$值减少了一个额外的值**

>正则化正规方程:

$$
\theta=(X^TX+\lambda\left[\begin{array}{ccccc}0 &\cdots &\cdots &\cdots &0 \\ 0 &1 &\cdots &\cdots &0\\ \vdots & \vdots & 1 &\cdots & 0\\ \vdots &\vdots &\cdots &\ddots & \vdots \\ 0 & 0 &\cdots &\cdots &1 \end{array}\right])^{-1}X^Ty
$$

## 正则化逻辑回归

逻辑回归的正则化的公式与线性回归类似，不过假设函数为$h_{\theta}(x)=g(\theta^{T}x)$。



