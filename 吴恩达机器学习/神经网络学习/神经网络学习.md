逻辑回归可以对非线性问题进行回归，不过对于特征比较多的分类问题来说逻辑回归就不是很适用了。对于高维特征的非线性分类问题，神经网络是典型的不需要增加特征数目就能完成非线性分类问题的模型。

## 神经网络概述

人工神经网络（ANN：Artificial Neural Network），简称神经网络（NN：Neural Network）。迄今为止，人工神经网络尚无统一定义，其实一种模拟了人体神经元构成的数学模型，依靠系统的复杂程度，通过调整内部大量节点之间相互连接的关系，从而达到处理信息的目的。

![pic1](pic/pic1.png)

上图显示了人工神经网络是一个分层模型，逻辑上可以分为三层：

- 输入层：输入层接收特征向量  x 。
- 输出层：输出层产出最终的预测  h 。
- 隐含层：隐含层介于输入层与输出层之间，之所以称之为隐含层，是因为当中产生的值并不像输入层使用的样本矩阵  X  或者输出层用到的标签矩阵  y  那样直接可见。

- 神经网络的表示

$$
a_{1}^{(2)} = g(\Theta_{10}^{(1)}x_{0} + \Theta_{11}^{(1)}x_{1}+ \Theta_{12}^{(1)}x_{2}+\Theta_{13}^{(1)}x_{3}) 
$$
$$
a_{2}^{(2)} = g(\Theta_{20}^{(1)}x_{0} + \Theta_{21}^{(1)}x_{1}+ \Theta_{22}^{(1)}x_{2}+\Theta_{23}^{(1)}x_{3}) 
$$
$$
a_{3}^{(2)} = g(\Theta_{30}^{(1)}x_{0} + \Theta_{31}^{(1)}x_{1}+ \Theta_{32}^{(1)}x_{2}+\Theta_{33}^{(1)}x_{3}) 
$$

$$
h_{\Theta}(x)= a_{1}^{(3)} = g(\Theta_{10}^{(2)}a_{0}^{(2)} + \Theta_{11}^{(2)}a_{1}^{(2)}+ \Theta_{12}^{(2)}a_{2}^{(2)}+\Theta_{13}^{(2)}a_{3}^{(2)}) 
$$

上面表达式中$a_{i}^{(j)}$代表第$j$的第$i$个激活单元，$\Theta(j)$代表第$j$层到第$j+1$层权重矩阵。

上述模型我们也可以使用向量来表示:

$$
x =\begin{bmatrix}
    x0 \\
    x1 \\
    x3\\
    x4
\end{bmatrix}
$$

$$
z^{(2)} = \begin{bmatrix}
    z_{1}^{(2)} \\
    z_{2}^{(2)} \\
    z_{3}^{(2)} \\
\end{bmatrix}
$$

$$
z^{(2)} = \Theta^{(1)}x
$$

$$

$$

> $\Theta = \begin{bmatrix}
     \Theta_{10}^{(1)},  \Theta_{11}^{(1)}, \Theta_{12}^{(1)}, \Theta_{13}^{(1)}
\end{bmatrix}$

>$z_{1}^{(2)} = \Theta_{10}^{(1)}x_{0} + \Theta_{11}^{(1)}x_{1}+ \Theta_{12}^{(1)}x_{2}+\Theta_{13}^{(1)}x_{3}$

