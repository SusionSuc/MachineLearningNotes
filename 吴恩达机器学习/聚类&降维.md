
简单回顾一下非监督学习: 在非监督学习中，我们需要将一系列无标签（$y$）的训练数据，输入到一个算法中，然后我们告诉这个算法，快去为我们找找这个数据的内在结构给定数据。

# 聚类 Clustering  (非监督学习算法)

聚类算法简单的说就是把数据在训练集中分成不同的簇。

## K-均值算法

它是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。

它的算法为:

1. 首先选择$K$个随机的点，称为聚类中心（cluster centroids）
2. 对于数据集中的每一个数据，按照距离$K$个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。
3. 计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。
4. 重复步骤2-3直至中心点不再变化。

用$\mu^{1},\mu^{2},...\mu^{n}$来表示聚类中心，用$c^{(1)},c^{(2)},...c^{(m)}$来存储与第$i$个实例数据最近的聚类中心的索引，K-均值算法的伪代码如下：

```
Repeat {
​
    for i = 1 to m
​
        c(i) := index (form 1 to K) of cluster centroid closest to x(i)
​
    for k = 1 to K
​
        μk := average (mean) of points assigned to cluster k

}
```

算法分为两个步骤，第一个**for**循环是赋值步骤，即：对于每一个样例$i$，计算其应该属于的类。第二个**for**循环是聚类中心的移动，即：对于每一个类$K$，重新计算该类的质心。

K-均值的代价函数为:

$$
J(c^{(1)},....c^{(m)},\mu_{1},...,\mu_{K}) = \frac{1}{m}\sum_{i=1}^{m}\lVert X^{(i)}-\mu_{c^{(i)}}\lVert^{2}
$$

K-均值迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。

# 降维 Dimensionality Reduction

降维也是属于无监督学习问题，它可以实现 **数据压缩&数据可视化**。

## 主成分分析 PCA (Principal Component Analysic)

在PCA中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。

![pic](pic/pca1.png)

假如要将$n$维数据降至$k$维，目标是找到向量$\mu_{1},...,\mu_{k}$使得总的投射误差最小。它与线性回归是两种不同的算法。主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测:

![pic](pic/pca2.png)

PCA将$n$个特征降维到$k$个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。

### PCA 算法实现

PCA的算法实现分为三步:

1. 均值归一化

需要计算出所有特征的均值，然后令$x_{j}=x_{j}-\mu_{j}$。如果特征是在不同的数量级上，我们还需要将其除以标准差$\sigma^{2}$。

2. 计算协方差矩阵

$$
\sum = \frac{1}{m}\sum_{i=1}^{n}(x^{(i)})(x^{(i)})^{T}
$$

什么是协方差矩阵呢?  具体看样参考这篇文章 : 

[如何直观地理解「协方差矩阵」](https://zhuanlan.zhihu.com/p/37609917)

3. 计算协方差矩阵$\sum$的特征向量

在`octave`里可以利用**奇异值分解**来求解:

$$
[U,S,V] = svd(sigma)
$$

$$
Sigma=\frac{1}{m}\sum_{i=1}^{n}(x^{(i)})(x^{(i)})^T
$$

通过`octave`计算后，对于一个$n*n$的矩阵,上式中的$U$是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从$n$维降至$k$维，我们只需要从中选取前$k$个向量，获得一个$n*k$维度的矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量$z^{(i)}$:$z^{(i)}=U_{reduce}^{T} * x^{(i)}$ 

>其中$x$是$n * 1$维的，因此结果为$k*1$维度.

## 选择主成分的数量 (确定要压缩到的维度)

主要成分分析是减少投射的平均均方误差:$\frac{1}{m}\sum_{i=1}^{m}\lVert x^{(i)}-x_{approx}^{(i)}\lVert^{2}$

训练集的方差为 : $\frac{1}{m}\sum_{i=1}^{m}\lVert x^{(i)} \lVert ^{2}$

**我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的值**

确定最终$k$的方案为:

我们可以先令$k=1$，然后进行主要成分分析，获得$U_{reduce}$和$z$，然后计算比例是否小于1%。如果不是的话再令$k=2$，如此类推，直到找到可以使得比例小于1%的最小$k$值（原因是各个特征之间通常情况存在某种相关性）。

## 重建的压缩表示

我们可以使用PCA对数据进行降维，那么怎么把降维后的数据变为原来的呢?

假如 $x$为2维,$z$为1维，$z = U_{reduce}^{T}x$, 则相反的方程为:

$$
x_{approx}=U_{reduce} \cdot z , x_{approx} \approx x
$$

## PCA的应用步骤

假使我们正在针对一张 100×100像素的图片进行某个计算机视觉的机器学习，即总共有10000个特征。

1. 第一步是运用主要成分分析将数据压缩至1000个特征
2. 然后对训练集运行学习算法
3. 在预测时，采用之前学习而来的$U_{reduce}$将输入的特征$x$转换成特征向量$z$，然后再进行预测

### 错误的使用方法

错误的主要成分分析情况：一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。

另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。