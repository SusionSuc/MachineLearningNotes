# 神经网络学习

## 模型表示

![pic1](pic/pic1.png)

$$
a_{1}^{(2)} = g(\Theta_{10}^{(1)}x_{0} + \Theta_{11}^{(1)}x_{1}+ \Theta_{12}^{(1)}x_{2}+\Theta_{13}^{(1)}x_{3}) 
$$
$$
a_{2}^{(2)} = g(\Theta_{20}^{(1)}x_{0} + \Theta_{21}^{(1)}x_{1}+ \Theta_{22}^{(1)}x_{2}+\Theta_{23}^{(1)}x_{3}) 
$$
$$
a_{3}^{(2)} = g(\Theta_{30}^{(1)}x_{0} + \Theta_{31}^{(1)}x_{1}+ \Theta_{32}^{(1)}x_{2}+\Theta_{33}^{(1)}x_{3}) 
$$

$$
h_{\Theta}(x)= a_{1}^{(3)} = g(\Theta_{10}^{(2)}a_{0}^{(2)} + \Theta_{11}^{(2)}a_{1}^{(2)}+ \Theta_{12}^{(2)}a_{2}^{(2)}+\Theta_{13}^{(2)}a_{3}^{(2)}) 
$$

上面表达式中$a_{i}^{(j)}$代表第$j$的第$i$个激活单元，$\Theta(j)$代表第$j$层到第$j+1$层权重矩阵。

上述模型我们也可以使用向量来表示:

$$
x =\begin{bmatrix}
    x0 \\
    x1 \\
    x3\\
    x4
\end{bmatrix}
$$

$$
z^{(2)} = \begin{bmatrix}
    z_{1}^{(2)} \\
    z_{2}^{(2)} \\
    z_{3}^{(2)} \\
\end{bmatrix}
$$

$$
z^{(2)} = \Theta^{(1)}x
$$

$$
Add \quad a_{0}^{(2)} = 1
$$

$$
z^{(3)} = \Theta^{(2)}a^{(2)}
$$

$$
h_{\Theta}(x) = a^{(3)} = g(z^{(3)})
$$

$$
z_{1}^{(2)} = \Theta_{10}^{(1)}x_{0} + \Theta_{11}^{(1)}x_{1}+ \Theta_{12}^{(1)}x_{2}+\Theta_{13}^{(1)}x_{3}
$$

## 代价函数

$$
J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2
$$

>这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出个预测，基本上我们可以利用循环，对每一行特征都预测个不同结果，然后在利用循环在个预测中选择可能性最高的一个，将其与中的实际数据进行比较。

>正则化的那一项只是排除了每一层后，每一层的 矩阵的和。最里层的循环循环所有的行（由 +1 层的激活单元数决定），循环则循环所有的列，由该层（层）的激活单元数所决定。即：与真实值之间的距离为每个样本-每个类输出的加和，对参数进行regularization的bias项处理所有参数的平方和。