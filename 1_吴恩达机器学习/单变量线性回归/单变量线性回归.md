
>本文是《吴恩达机器学习课程第二章》的简单笔记 

# 引入问题

>这里有一个关于房子尺寸和价格的数据集:

|  |  |  |||||
| :-----| ----: | :----: | ----: | :----: |----: | :----: |
| x (size,平方) | 1000| 1100 |2000|2400|3000|...|
| y (price,万) | 130 | 125 |230|260|240|...|

具体数据分布如下图:

![pic_1](pic_1.png)

根据这些数据,我们可以找到一条曲线(二次方程)来表示**房子尺寸x和价格y**的关系:

`hθ(x) = θ0 + θ1 * x`

那么`hθ(x)`在什么时候最接近样本集呢 ？

>最接近的应该是对于每一个样本 `h(x_i)`近视相等于`y_i `, 那这时`θ_0、θ_1`等于多少呢？

即我们的目标:

**就是求`θ_0、θ_1`的最合理的值，来使任意样本(x_i, y_i)的`hθ(x_i) - y_i`近可能得相等)**

# 代价函数

怎么求呢? 这里要引入代价函数的概念：

>简单的来说代价函数就是可以预测出`hθ(x_i)`与真实样本值`y_i`之间的差异的函数

这里`hθ(x) = θ0 + θ1 * x`的代价函数是:

![pic_2](pic_2.png)

>这里`m代表样本集的数量`, 大致的意思就是取不同`θ0和θ1`时`hθ(x)`与`y_i`的差值。

那这个函数是怎么推导出来的呢？ 

>暂时我还不知道。。。不过这里可以回顾一下我们`平均值(Mean)、方差(Variance)、标准差(Standard Deviation)`这个公式和方差的公式比较像。具体可以参考这篇文章: http://blog.shaochuancs.com/mean-variance-sd/

我们可以取`m=3`，然后取不同的`θ0和θ1`作图，这个函数其实是一个三维的函数,直接看可能不是很好理解。这里我们先假设`θ0 = 0`,即变为:

这样其实就是`J(θ1)`与`θ1`变化的函数:

![pic_3](pic_3.png)

取`m=3`,对上面这个函数求值，画出这个函数大约是下面这个样子:

![pic_4](pic_4.png)

通过上图可以看出在`θ1=1`时`J(θ_1)`最小,此时对于每个样本`hθ(x_i)`与`y_i`的值最接近。如果只有`θ1`这一个变量的话，其实此时`θ1=1`就是最符合的取值，因为它可以使`hθ(x_i)`与`y_i`的值最接近。

那如果再加上一个`θ_0`呢？`J(θ0,θ1)`的数学曲线其实就变成了下面这个样子:

![pic_5](pic_5.png)


即类似于一个碗状的图形。参照上面的二维曲线，其实此时**使`hθ(x_i)`与`y_i`的值最接近**的`θ0和θ1`的取值应该是这个碗状图最凹的地方。

对于这个`碗状图`我可以使用`等高线`图来理解，它的等高线图如下图所示:

![pic_6.png](pic_6.png)

>`等高线`图的大概意思就是分别取不同的`θ0和θ1`,把所有相等的`J(θ0,θ1)`点连接起来。

`J(θ0,θ1)`的`等高线`图类似于一个逐渐收敛的圆环，取不同的`θ0和θ1`它所对应的`hθ(x)`是不同的，结合上面的`3D碗状图形`，其实使`hθ(x)`与`y_i`最接近的点就是`3D碗状图的最低点`和`等高线图的最收敛处`,其实就是下面这个点，取这个点的`θ0和θ1`时，它对应的`hθ(x)`的曲线是这样的:

![pic_7.png](pic_7.png)

OK，到目前为止我们已经在图形上找到了目标的`θ0和θ1`,那么我们如何具体求出它们的值呢？？？

# 梯度下降

这里我们要引入**梯度下降**的概念。梯度下降的算法大概可以这样理解:

>梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。

具体解释可以参考这篇博客(**务必看一下**): https://www.jianshu.com/p/c7e642877b0e  

具体的下山过程如下图所示:

![pic_8](pic_8.png)

>那么如何在每次下降时确定往哪里走呢？即最陡峭的地方怎么算呢？

我们可以使用**梯度下降**算法，这里直接给出`J(θ0,θ1)`的梯度下降函数:

![pic_9](pic_9.png)

大体是个什么意思呢？

**我们通过这个`梯度下降函数`不断更新`θ0和θ1`的值，直到他们不能再更新为止, 那么此时的`θ0和θ1`的取值就是我们要找的目标值**(说的有点不明白- - )

>- `:= `可以简单的理解我我们编程中的赋值符号
>- `α`叫做学习速率，它越大，梯度下降的速率就越快 

为了理解`J(θ0,θ1)`的梯度下降，我们这里先假设`θ0 = 0`,那么`梯度下降`函数就变为:

![pic_10](pic_10.png)

上面这个函数的![pic_11](pic_11.png)部分,可以理解为`J(θ1)`的导数，导数其实可以理解为某点直线的**斜率**

>对于导数的理解可以参考这个知乎的回答: https://www.zhihu.com/question/28684811

我们对`J(θ1)`做图，然后取两个不同的`θ1`点，再套用梯度下降的公式来更新`θ1`的值发现，新的`θ1`的值是不断在靠近`J(θ1)`的最低点的，即使用**梯度下降算法来下山是正确的！！**

![pic_12](pic_12.png)

上图在什么时候`θ1`不能继续被更新呢？ 其实就是在最低点，此时:

`θj = θj - 0`

0就是`J(θ1)`在最低点时的导数，即斜率！

通过对`J(θ1)`进行梯度下降我们求得了`θ1`的值。 同理我们对`J(θ0,θ1)`进行梯度下降一样可以求得`θ0和θ1`的值。此时他们就可以使`J(θ0,θ1)`最小，也就是对于任意样本(x_i, y_i)的`hθ(x_i) - y_i`都尽可能相等，即我们求出了最佳拟合曲线`hθ(x)`:

![pic_13](pic_13.png)


$$
\theta_{j} := \theta_{j} - \alpha \sum_{i=1}^{m}(h_{\theta}(x^{i} - y^{(i)})x_{j}^{(i)}
$$


# 总结

其实整个求`θ0和θ1`的思路并不复杂:

1. 先求出`hθ(x)`的代价函数`J(θ0,θ1)`
2. 对代价函数`J(θ0,θ1)`进行梯度下降，求出最使`J(θ0,θ1)`最小的点
3. 目标`θ0和θ1`的值就求出来了。

这里由于我的数学知识都快还给老师了，所以对于代价函数、梯度下降算法所涉及的方差、微积分、导数等都不怎么记得了，因此这两个函数就不是很好理解。那办法是什么呢？ **努力理解呗(O(∩_∩)O哈哈~....)**


推荐阅读

> https://www.jianshu.com/p/e6b420b82460